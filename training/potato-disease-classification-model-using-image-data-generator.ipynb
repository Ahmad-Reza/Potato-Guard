{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac236949-1291-4af5-951e-63c3bebf590e",
   "metadata": {},
   "source": [
    "## Potat Disease Classification\n",
    "\n",
    "#### Import all the Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a78a807-7d13-4f57-89b3-b1ff18264dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "852c4a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ecac71-5541-4366-9d4e-264b816e49d6",
   "metadata": {},
   "source": [
    "## Import data into tensorflow dataset object\n",
    "\n",
    "#### Used splitfolders tool to split dataset into training, validation and test directories.\n",
    "#### $ pip install split-folders\n",
    "\n",
    "#### $ splitfolders --ratio 0.8 0.1 0.1 -- ./training/PlantVillage/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfa89631-edd2-4f95-bdce-4ec788757168",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256\n",
    "CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78976aaa-99fd-4372-9eff-3cf60e591210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1506 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    '../dataset/train',\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=32,\n",
    "    class_mode=\"sparse\",\n",
    " #         save_to_dir=\"C:\\\\Code\\\\potato-disease-classification\\\\training\\\\AugmentedImages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9140bab-503c-4c97-bd5c-84d84885b6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Potato___Early_blight': 0, 'Potato___Late_blight': 1, 'Potato___healthy': 2}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ed143e3-c707-4129-b934-f5af2a78b16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = list(train_generator.class_indices.keys())\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5356f096-3859-41e8-80f6-3cc1fa381d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.49095592 0.48703435 0.5497795 ]\n",
      "  [0.4815682  0.4745091  0.53882295]\n",
      "  [0.49630326 0.48453856 0.5512052 ]\n",
      "  ...\n",
      "  [0.6498323  0.64591074 0.716499  ]\n",
      "  [0.65321887 0.6492973  0.7198855 ]\n",
      "  [0.6566054  0.65268385 0.7232721 ]]\n",
      "\n",
      " [[0.49157164 0.48765007 0.5503952 ]\n",
      "  [0.48249182 0.47604844 0.54005444]\n",
      "  [0.4919931  0.4802284  0.5468951 ]\n",
      "  ...\n",
      "  [0.6785408  0.67461926 0.7452075 ]\n",
      "  [0.67730933 0.67338777 0.743976  ]\n",
      "  [0.67607784 0.6721563  0.7427445 ]]\n",
      "\n",
      " [[0.49218738 0.4882658  0.55101097]\n",
      "  [0.48341542 0.47758776 0.5412859 ]\n",
      "  [0.48768294 0.4759182  0.5425849 ]\n",
      "  ...\n",
      "  [0.6591392  0.65521765 0.7258059 ]\n",
      "  [0.6566763  0.6527547  0.72334296]\n",
      "  [0.6542133  0.65029174 0.72088   ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.6256553  0.6217337  0.69232196]\n",
      "  [0.61980575 0.6158842  0.6864724 ]\n",
      "  [0.6139563  0.6100347  0.68062294]\n",
      "  ...\n",
      "  [0.672628   0.67654955 0.7471378 ]\n",
      "  [0.69232106 0.69624263 0.76683086]\n",
      "  [0.71282136 0.71674293 0.78733116]]\n",
      "\n",
      " [[0.60078406 0.5968625  0.6674507 ]\n",
      "  [0.6013998  0.5974782  0.66806644]\n",
      "  [0.6020155  0.5980939  0.66868216]\n",
      "  ...\n",
      "  [0.67201227 0.67593384 0.74652207]\n",
      "  [0.69047385 0.6943954  0.76498365]\n",
      "  [0.711282   0.7152036  0.7857918 ]]\n",
      "\n",
      " [[0.5898775  0.5859559  0.65654415]\n",
      "  [0.5880303  0.5841087  0.65469694]\n",
      "  [0.5861831  0.5822615  0.65284973]\n",
      "  ...\n",
      "  [0.6713965  0.67531806 0.7459063 ]\n",
      "  [0.6886266  0.69254816 0.7631364 ]\n",
      "  [0.70974267 0.71366423 0.7842525 ]]]\n"
     ]
    }
   ],
   "source": [
    "for image_batch, label_batch in train_generator:\n",
    "    print(image_batch[0])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d670337a-c816-4f4f-864c-c98ad9eef7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 215 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    '../dataset/val',\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=32,\n",
    "    class_mode=\"sparse\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba3899a9-d4b2-4446-b625-61668f1f39f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 431 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    '../dataset/test',\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=32,\n",
    "    class_mode=\"sparse\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cab3d05a-8d4b-490d-80cb-4f3c8a06b341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.45095715 0.38429046 0.4078199 ]\n",
      "  [0.51567453 0.44900784 0.47253725]\n",
      "  [0.5274629  0.46079627 0.48432568]\n",
      "  ...\n",
      "  [0.43875483 0.3799313  0.39953914]\n",
      "  [0.4644276  0.40560406 0.4252119 ]\n",
      "  [0.43411535 0.37529182 0.39489967]]\n",
      "\n",
      " [[0.44334006 0.3766734  0.4002028 ]\n",
      "  [0.46898383 0.40231714 0.42584655]\n",
      "  [0.46686476 0.4001981  0.4237275 ]\n",
      "  ...\n",
      "  [0.44275582 0.3839323  0.40354013]\n",
      "  [0.4585303  0.39970678 0.41931462]\n",
      "  [0.4351016  0.37627807 0.3958859 ]]\n",
      "\n",
      " [[0.4570543  0.39038762 0.41391703]\n",
      "  [0.447639   0.38097233 0.40450174]\n",
      "  [0.43001533 0.36334866 0.38687807]\n",
      "  ...\n",
      "  [0.4767833  0.41795978 0.43756762]\n",
      "  [0.4620002  0.40317667 0.4227845 ]\n",
      "  [0.42333692 0.3645134  0.38412124]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.70999646 0.6707808  0.67470235]\n",
      "  [0.7176904  0.6784747  0.6823963 ]\n",
      "  [0.72596097 0.6867453  0.69066685]\n",
      "  ...\n",
      "  [0.67556465 0.6324274  0.6559568 ]\n",
      "  [0.6672246  0.62408733 0.64761674]\n",
      "  [0.67152065 0.6283834  0.6519128 ]]\n",
      "\n",
      " [[0.7084326  0.6692169  0.67313844]\n",
      "  [0.7144625  0.67524683 0.6791684 ]\n",
      "  [0.7194127  0.680197   0.68411857]\n",
      "  ...\n",
      "  [0.6679682  0.62483096 0.6483604 ]\n",
      "  [0.66197604 0.6188388  0.6423682 ]\n",
      "  [0.6718514  0.62871414 0.65224355]]\n",
      "\n",
      " [[0.71490437 0.6756887  0.67961025]\n",
      "  [0.71669805 0.67748237 0.68140393]\n",
      "  [0.71429753 0.67508185 0.6790034 ]\n",
      "  ...\n",
      "  [0.6499941  0.6068568  0.63038623]\n",
      "  [0.64568746 0.6025502  0.6260796 ]\n",
      "  [0.65980977 0.6166725  0.6402019 ]]]\n"
     ]
    }
   ],
   "source": [
    "for image_batch, label_batch in test_generator:\n",
    "    print(image_batch[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee850bdf-2445-4100-a871-4a8891977073",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7021ac7-9cb9-42c0-8a20-e4642c464231",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
    "n_classes = 3\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.InputLayer(input_shape=input_shape),\n",
    "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(n_classes, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34dd9289-8c6e-430f-9e39-6bba85a3fe7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 254, 254, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 127, 127, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 125, 125, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 62, 62, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 60, 60, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 30, 30, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 28, 28, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 12, 12, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 2, 2, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183,747\n",
      "Trainable params: 183,747\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0430de1d-7cec-464c-b005-602f61d1bcd1",
   "metadata": {},
   "source": [
    "### Compiling the Model\n",
    "\n",
    "#### We use adam Optimizer, SparseCategoricalCrossentropy for losses, accuracy as a metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf59dbac-23b7-4ff1-88ba-907839091932",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2910ec9-98d3-4c7a-a87e-92dc674593f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.0625"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1506/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "509aaa63-4eb3-4824-84ca-dd667cf36fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.71875"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "215/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d4fc64-2628-435b-b755-bff5c33e7595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30/47 [==================>...........] - ETA: 15s - loss: 0.9087 - accuracy: 0.4903"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=47,\n",
    "    batch_size=32,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=6,\n",
    "    verbose=1,\n",
    "    epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8476eae1-f389-441b-a97b-901ef815fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb0ca03-2a58-4f71-ac5a-7af78a51dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores is just a list containing loss and accuracy value\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac79ead-4028-4e2c-a791-6f1ea89de263",
   "metadata": {},
   "source": [
    "### Plotting the Accuracy and Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f7dc21-3b26-48de-8c89-3bd224c42e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4124f24a-3a81-4119-85b6-2baee25273bb",
   "metadata": {},
   "source": [
    "#### You can read documentation on history object here: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfb4888-e25e-4a53-be2c-5629ad3e339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6ea177-7aaf-47d9-9d7b-1d6298b7e43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc00a028-413d-445e-96e8-777d6fec1d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79d8cec-e446-4bd2-8c1d-f1e0b8b12824",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history['loss'][:5] # show loss for first 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35458c74-d042-4888-983a-55ab6bfb7eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce1661f-1135-4fa6-a72e-9c486ef6f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5898b5-2e8e-44ee-ba0f-7ea2d174865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb9e2b1-e59c-4788-89f9-d54461027dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(EPOCHS), acc, label='Training Accuracy')\n",
    "plt.plot(range(EPOCHS), val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(EPOCHS), loss, label='Training Loss')\n",
    "plt.plot(range(EPOCHS), val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ee50e5-23f6-4ef5-8b77-9aa00d032317",
   "metadata": {},
   "source": [
    "### Run prediction on sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58029b38-6940-47dc-b392-900fae8018b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for image_batch, label_batch in test_generator:\n",
    "    first_image = image_batch[0]\n",
    "    first_label = label_batch[0]\n",
    "\n",
    "    print('first image to predict')\n",
    "    plt.imshow(first_image)\n",
    "    print(f'actual label: {class_names[int(first_label)]}')\n",
    "\n",
    "    batch_prediction = model.predict(image_batch)\n",
    "    print(f'predicted label: {class_names[np.argmax(batch_prediction[0])]}')\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3ebfd3-7187-4163-b4fe-3e7a404f050e",
   "metadata": {},
   "source": [
    "### Write a function for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d718fd43-335f-40df-9f6d-796e22f752c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, img):\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(images[i])\n",
    "    img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "\n",
    "    predicted_class = class_names[np.argmax(predictions[0])]\n",
    "    confidence = round(100 * (np.max(predictions[0])), 2)\n",
    "    return predicted_class, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bffc95b-4081-45d4-9214-7ac6dd7020f6",
   "metadata": {},
   "source": [
    "#### Now run inference on few sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f7a36c-8697-420b-99d1-a1a2d7b2dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "for images, labels in test_generator:\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i])\n",
    "\n",
    "        predicted_class, confidence = predict(model, images[i])\n",
    "        actual_class = class_names[int(labels[i])]\n",
    "\n",
    "        plt.title(f\"Actual: {actual_class} \\n predicted:{predicted_class} \\n confidence: {confidence}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa93594-0d8a-4eec-9198-44fd0ec18f9b",
   "metadata": {},
   "source": [
    "## Saving the Model\n",
    "#### Save model in h5 format so that there is just one file and we can upload that to GCP conveniently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9ad03f-cd94-4c8d-a287-4a71eb4f9c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../models/potatoes.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47aef123-69ae-439a-9623-2cd3a6b58fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
